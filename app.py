import streamlit as st
import uuid
import chromadb
import os
import re
import time
from styles import load_custom_css

from DocumentProcessor import (
    get_files_from_folder,
    get_file_hash,
    load_cache,
    extract_pdf_detailed,
    extract_docx_detailed,
    extract_txt_detailed,
    save_cache
)
from ChatEngine import get_embedding_function, answer_question_with_groq,expand_query_multilingual

st.set_page_config(
    page_title="Biomedical Document Chatbot",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

load_custom_css()

DOCS_FOLDER = "/mount/src/lasst/documents"
CACHE_FOLDER = os.getenv("CACHE_FOLDER", "./cache")
CHROMA_FOLDER = "./chroma_db"

os.makedirs(DOCS_FOLDER, exist_ok=True)
os.makedirs(CACHE_FOLDER, exist_ok=True)
os.makedirs(CHROMA_FOLDER, exist_ok=True)

if "collection" not in st.session_state:
    st.session_state.collection = None

if "chats" not in st.session_state:
    st.session_state.chats = {}
    st.session_state.active_chat = None

client = chromadb.PersistentClient(path=CHROMA_FOLDER)  

collections = client.list_collections()

if collections:
    collection = client.get_collection(
        name=collections[0].name,
        embedding_function=get_embedding_function()
    )
    st.session_state.collection = collection
else:
    with st.spinner("ğŸ“š Processing documents... This may take a while for the first time."):
        files = get_files_from_folder()
        st.write("ğŸ” Searching for documents in:")
        st.code(DOCS_FOLDER)
        st.write(f"Found {len(files)} files:")
        if files:
            for f in files:
                st.write(f"- {f}")
        else:
            st.error("ğŸš¨ No files found! Check the folder path and file extensions.")
            st.stop()
        if not files:
            st.error("No documents found in the documents folder!")
            st.stop()

        collection = client.create_collection(
            name="biomed_docs",
            embedding_function=get_embedding_function(),
            metadata={"hnsw:space": "cosine"}  
        )

        all_chunks = []
        all_meta = {}
        all_ids = []

        processed_files = []  
        processed_count = 0  

        for idx, path in enumerate(files):
            name = os.path.basename(path)
            ext = name.split(".")[-1].lower()
            key = f"{get_file_hash(path)}_{ext}"
            cached = load_cache(key)

            if cached:
                info = cached
                st.success(f"âœ… Loaded from cache: {name}") 
            else:
                st.info(f"ğŸ”„ Processing: {name} ...") 
                if ext == "pdf":
                    info, error = extract_pdf_detailed(path)
                elif ext in ["doc", "docx"]:
                    info, error = extract_docx_detailed(path)
                elif ext == "txt":
                    info, error = extract_txt_detailed(path)
                else:
                    st.warning(f"âš ï¸ Skipped unsupported file: {name}")
                    continue
                if error:
                    st.warning(f"âš ï¸ Error in {name}: {error}")
                    continue
                save_cache(key, info)

            st.success(f"âœ… Processed successfully: {name}")
            processed_files.append(name)
            processed_count += 1

            for c in info["chunks"]:
                all_chunks.append(c["content"])
                all_meta[len(all_chunks) - 1] = c["metadata"]
                all_ids.append(f"chunk_{idx}_{len(all_chunks)}")

        if processed_count > 0:
            st.success(f"ğŸ‰ All done! Processed {processed_count} documents successfully!")
            st.write("**Processed documents:**")
            for file_name in processed_files:
                st.write(f"- {file_name}")
        else:
            st.error("âŒ No documents were processed!")

        batch_size = 300
        for i in range(0, len(all_chunks), batch_size):
            collection.add(
                documents=all_chunks[i:i+batch_size],
                metadatas=[all_meta[j] for j in range(i, min(i+batch_size, len(all_chunks)))],
                ids=all_ids[i:i+batch_size]
            )

        st.session_state.collection = collection
        st.success(f"âœ… Processed {len(files)} documents successfully!")

if not st.session_state.chats:
    cid = f"chat_{uuid.uuid4().hex[:6]}"
    st.session_state.chats[cid] = {
        "title": "New Chat",
        "messages": [],
        "context": []
    }
    st.session_state.active_chat = cid

st.markdown("""
<div class="main-card">
    <h1 style='text-align:center;margin:0;'>ğŸ§¬ Biomedical Document Chatbot</h1>
</div>

<div class="main-card">
    <p style="text-align:center; font-size:1.1rem;">Answers <strong>only</strong> from official documents â€¢ Supports English, German & Arabic â€¢ Remembers conversation</p>
    <h3 style="color:#00d9ff;">Try these examples:</h3>
    <ul style="font-size:1.05rem;">
        <li>What are the requirements for registering the master's thesis?</li>
        <li>Tell me about the internship requirements</li>
        <li>Was sind die Regelungen fÃ¼r die Masterarbeit?</li>
    </ul>
</div>
""", unsafe_allow_html=True)

with st.sidebar:
    st.markdown("# ğŸ§¬ BioMed Chat")

    if st.button("â• New Chat", use_container_width=True, type="primary"):
        cid = f"chat_{uuid.uuid4().hex[:6]}"
        st.session_state.chats[cid] = {
            "title": "New Chat",
            "messages": [],
            "context": []
        }
        st.session_state.active_chat = cid
        st.rerun()

    st.markdown("### ğŸ’¬ Your Chats")
    for cid in reversed(list(st.session_state.chats.keys())):   
        chat = st.session_state.chats[cid]
        col1, col2 = st.columns([4, 1])
        with col1:
            if st.button(f"ğŸ’¬ {chat['title'][:35]}...", key=f"open_{cid}", use_container_width=True):
                st.session_state.active_chat = cid
                st.rerun()
        with col2:
            if st.button("ğŸ—‘ï¸", key=f"del_{cid}"):
                del st.session_state.chats[cid]
                if st.session_state.active_chat == cid:
                    st.session_state.active_chat = next(iter(st.session_state.chats), None)
                st.rerun()

chat = st.session_state.chats[st.session_state.active_chat]
for m in chat["messages"]:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

if query := st.chat_input("Ask anything about the MBE program documents..."):
    chat["messages"].append({"role": "user", "content": query})
    if chat["title"] == "New Chat":
        chat["title"] = query[:40] + "..." if len(query) > 40 else query

    with st.chat_message("user"):
        st.markdown(query)

    with st.chat_message("assistant"):
        with st.spinner("Searching documents & thinking..."):
            queries = expand_query_multilingual(
                query,
                st.session_state.collection
            )

            res = st.session_state.collection.query(
                query_texts=queries,
                n_results=15
            )
            chunks = []
            for docs, metas in zip(res["documents"], res["metadatas"]):
                for d, m in zip(docs, metas):
                    chunks.append({
                        "content": d,
                        "metadata": m
                    })

            answer, used_chunks = answer_question_with_groq(query, chunks, chat["messages"])
            st.markdown(answer)

            match = re.search(r'wait (\d+) seconds', answer.lower())
            if match:
                remaining = int(match.group(1))
                countdown = st.empty()
                while remaining > 0:
                    countdown.warning(f"â³ Please wait {remaining} seconds before sending a new request...")
                    time.sleep(1)
                    remaining -= 1
                countdown.success("âœ… You can now send a new question.")

            st.markdown("### ğŸ“š Answer was based on the following document excerpts:")
            for i, ch in enumerate(used_chunks, 1):
                with st.expander(f"ğŸ“„ {ch['source']} â€” Page {ch['page']}"):
                    st.markdown(ch["content"])

    chat["messages"].append({"role": "assistant", "content": answer})
    chat["context"] = chunks
    

